{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ebcc3f6",
   "metadata": {},
   "source": [
    "# Utility Functions\n",
    "\n",
    "This notebook contains utility functions which I found useful for my research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84621650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90a311f",
   "metadata": {},
   "source": [
    "## ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61f870e",
   "metadata": {},
   "source": [
    "### Accuracy in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2826e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y_true, y_prob):\n",
    "    y_pred = y_prob.argmax(1)\n",
    "    correct = (y_pred == y_true).type(torch.float).sum().item()\n",
    "    return correct / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ab6dc6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_prob = torch.randn(3, 2)\n",
    "y_prob = nn.Softmax(dim=-1)(y_prob)\n",
    "y_true=torch.empty(3, dtype=torch.long).random_(2)\n",
    "my_acc = get_accuracy(y_true, y_prob)\n",
    "\n",
    "y_pred = y_prob.argmax(1)\n",
    "exp_acc = accuracy_score(y_true, y_pred)\n",
    "assert my_acc == exp_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce410f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0887, 0.1730, 0.2687, 0.0590, 0.4106],\n",
      "        [0.2118, 0.2614, 0.0332, 0.1918, 0.3017],\n",
      "        [0.0838, 0.1923, 0.1487, 0.3859, 0.1893]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(ip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7621b455",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000fc84d",
   "metadata": {},
   "source": [
    "### BERT Token Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "627ae892",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tuple' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/h3/fxy3m4sj5tq7_q6n45n5gnmh0000gn/T/ipykernel_6464/1836967896.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_subspan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubspan\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspan\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubspan\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Tuple' is not defined"
     ]
    }
   ],
   "source": [
    "def get_token_embeddings(string, tokenizer, embedding_model, merge_strategy = \"average\"):\n",
    "    \"\"\" Retrieves token embeddings by accumulating wordpiece embeddings based on merge strategy.\n",
    "    Identify wordpiece to tokens by checking if their character span is in subset of the original token char span. \"\"\"\n",
    "    def _merge_embeddings(wp_e, stack):\n",
    "        if merge_strategy == \"average\": \n",
    "            t_e = torch.mean(wp_e[stack], 0)\n",
    "        elif merge_strategy == \"first\":\n",
    "            t_e = wp_e[stack[0]]\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return t_e\n",
    "\n",
    "\n",
    "    inputs = tokenizer(string, truncation=True, return_tensors=\"pt\", add_special_tokens = False)\n",
    "    wp_e = get_wordpiece_embeddings(inputs, embedding_model)\n",
    "\n",
    "    ws_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "    token_spans = ws_tokenizer.span_tokenize(string)\n",
    "\n",
    "    # merging wordpiece embeddings\n",
    "    result = []\n",
    "    stack = []  # initialise stack with idx\n",
    "\n",
    "    t_span = next(token_spans)\n",
    "    for i in range(len(wp_e)):\n",
    "        wp_span = inputs.token_to_chars(i)\n",
    "        if is_subspan(wp_span, t_span): stack.append(i)\n",
    "        else: \n",
    "            t_e = _merge_embeddings(wp_e, stack)\n",
    "            result.append(t_e)\n",
    "            t_span = next(token_spans) # if error is thrown, sth is wrong as every wp should be a subspan of some token\n",
    "\n",
    "            assert is_subspan(wp_span, t_span)\n",
    "            stack = [i]  # initialise stack with current idx\n",
    "    \n",
    "    # clear remaining accumulated tensors\n",
    "    t_e = _merge_embeddings(wp_e, stack)\n",
    "    result.append(t_e)\n",
    "\n",
    "    result = torch.stack(result, dim = 0)\n",
    "    assert len(result) == len(string.split()), f\"{len(result)} != {len(string.split())}\"\n",
    "    return result\n",
    "\n",
    "def is_subspan(subspan: Tuple[int], span: Tuple[int]) -> bool:\n",
    "    assert len(subspan) == 2\n",
    "    assert len(span) == 2\n",
    "    return subspan[0] >= span[0] and subspan[1] <= span[1]\n",
    "\n",
    "def get_model_device(model):\n",
    "    return next(model.parameters()).device\n",
    "\n",
    "def get_wordpiece_embeddings(inputs: Dict, model) -> Tensor:\n",
    "    \"\"\" Based on https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/#2-input-formatting \"\"\"\n",
    "    with torch.no_grad():\n",
    "        inputs = {k: v.to(get_model_device(model)) for k, v in inputs.items()}\n",
    "        outputs = model(**inputs)\n",
    "        hidden_states = outputs[2]  # 13x1xtx768\n",
    "\n",
    "    # stack list of tensors \n",
    "    wordpiece_embeddings = torch.stack(hidden_states[-4:], dim = 0)  # 4x1xtx768\n",
    "\n",
    "    # remove batch dimension\n",
    "    wordpiece_embeddings = torch.squeeze(wordpiece_embeddings, dim = 1)  # 4xtx768\n",
    "\n",
    "    # order by wordpiece tokens\n",
    "    wordpiece_embeddings = wordpiece_embeddings.permute(1, 0, 2)  # tx4x768\n",
    "\n",
    "    # concat last four hidden layers as in original paper\n",
    "    s = wordpiece_embeddings.size()\n",
    "    wordpiece_embeddings = wordpiece_embeddings.reshape(s[0], s[1] * s[2])  # tx3072\n",
    "    \n",
    "    return wordpiece_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "628d5b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_get_wordpiece_embeddings():\n",
    "    from transformers import AutoModel\n",
    "    from transformers import AutoTokenizer\n",
    "    \"\"\" Tests for contextual embeddings. Follows https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/#2-input-formatting. \"\"\"\n",
    "\n",
    "    from scipy.spatial.distance import cosine\n",
    "\n",
    "    # model_name = 'bert-base-uncased'\n",
    "    model_name = 'bert-base-multilingual-cased'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name, output_hidden_states = True)\n",
    "    model.eval()\n",
    "    \n",
    "    # english\n",
    "    en = \"After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.\"\n",
    "    inputs = tokenizer(en, return_tensors=\"pt\", truncation=True, add_special_tokens=False)\n",
    "    we_en = get_wordpiece_embeddings(inputs, model)\n",
    "\n",
    "    tokens = tokenizer.tokenize(en, truncation=True, add_special_tokens=False)\n",
    "    assert len(we_en) == len(tokens)\n",
    "    ids = [i for i, x in enumerate(tokens) if x == \"bank\"]\n",
    "\n",
    "    same_bank = 1 - cosine(we_en[ids[0]], we_en[ids[1]])\n",
    "    diff_bank = 1 - cosine(we_en[ids[0]], we_en[ids[2]])\n",
    "    print(f\"diff_bank vs same_bank for en: {diff_bank} vs {same_bank}\")\n",
    "    assert same_bank > diff_bank\n",
    "\n",
    "def test_get_token_embeddings():\n",
    "    from transformers import AutoModel\n",
    "    from transformers import AutoTokenizer\n",
    "    \"\"\" Tests for contextual embeddings. Follows https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/#2-input-formatting. \"\"\"\n",
    "\n",
    "    from scipy.spatial.distance import cosine\n",
    "\n",
    "    # model_name = 'bert-base-uncased'\n",
    "    model_name = 'bert-base-multilingual-cased'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name, output_hidden_states = True)\n",
    "    model.eval()\n",
    "    \n",
    "    # english\n",
    "    en = \"After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.\"\n",
    "    t_en = get_token_embeddings(en, tokenizer, model, merge_strategy=\"first\")\n",
    "\n",
    "    ids = [i for i, x in enumerate(en.split()) if \"bank\" in x]\n",
    "\n",
    "    same_bank = 1 - cosine(t_en[ids[0]], t_en[ids[1]])  # cosine similarity\n",
    "    diff_bank = 1 - cosine(t_en[ids[0]], t_en[ids[2]])\n",
    "    print(f\"diff_bank vs same_bank for en: {diff_bank} vs {same_bank}\")\n",
    "    assert same_bank > diff_bank\n",
    "\n",
    "    inputs = tokenizer(en, return_tensors=\"pt\", truncation=True, add_special_tokens=False)\n",
    "    we_en = get_wordpiece_embeddings(inputs, model)\n",
    "\n",
    "    tokens = tokenizer.tokenize(en, truncation=True, add_special_tokens=False)\n",
    "    assert len(we_en) == len(tokens)\n",
    "    we_ids = [i for i, x in enumerate(tokens) if x == \"bank\"]\n",
    "    # token and wordpiece embeddings should be same since \"bank\" is not split further.\n",
    "    assert torch.equal(t_en[ids[0]], we_en[we_ids[0]])\n",
    "    assert torch.equal(t_en[ids[1]], we_en[we_ids[1]])\n",
    "    assert torch.equal(t_en[ids[2]], we_en[we_ids[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8eaf1317",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/h3/fxy3m4sj5tq7_q6n45n5gnmh0000gn/T/ipykernel_6464/249485214.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_get_wordpiece_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_get_token_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/h3/fxy3m4sj5tq7_q6n45n5gnmh0000gn/T/ipykernel_6464/2050594619.py\u001b[0m in \u001b[0;36mtest_get_wordpiece_embeddings\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_get_wordpiece_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"\"\" Tests for contextual embeddings. Follows https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/#2-input-formatting. \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "test_get_wordpiece_embeddings()\n",
    "test_get_token_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09c8a27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
