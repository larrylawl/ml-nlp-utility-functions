{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ebcc3f6",
   "metadata": {},
   "source": [
    "# Utility Functions\n",
    "\n",
    "This notebook contains utility functions which I found useful for my research. Here's a quick summary:\n",
    "\n",
    "### ML\n",
    "1. Accuracy in Pytorch\n",
    "2. Precision, Recall, F1 in Pytorch\n",
    "3. Setting seeds\n",
    "\n",
    "### NLP\n",
    "1. Gradient Clipping\n",
    "2. BERT Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84621650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90a311f",
   "metadata": {},
   "source": [
    "## ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73f1a90",
   "metadata": {},
   "source": [
    "### Accuracy in PyTorch\n",
    "Used pytorch instead of sklearn to avoid converting tensor from gpu to cpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7ae9e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y_true, y_prob):\n",
    "    y_pred = y_prob.argmax(1)\n",
    "    correct = (y_pred == y_true).type(torch.float).sum().item()\n",
    "    return correct / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d380bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_prob = torch.randn(3, 2)\n",
    "y_prob = nn.Softmax(dim=-1)(y_prob)\n",
    "y_true=torch.empty(3, dtype=torch.long).random_(2)\n",
    "my_acc = get_accuracy(y_true, y_prob)\n",
    "\n",
    "y_pred = y_prob.argmax(1)\n",
    "exp_acc = accuracy_score(y_true, y_pred)\n",
    "assert my_acc == exp_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7311f7",
   "metadata": {},
   "source": [
    "### Precision, Recall, F1 in PyTorch\n",
    "Used pytorch instead of sklearn to avoid converting tensor from gpu to cpu. For my experiments, this led to an ~eyeballed~ estimated average speedup for 1s/it.\n",
    "\n",
    "Full credits to [this stackoverflow post](https://stackoverflow.com/questions/62265351/measuring-f1-score-for-multiclass-classification-natively-in-pytorch). I've simply exposed precision and recall in addition to f1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e156157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "class PRFScore:\n",
    "    \"\"\"\n",
    "    Class for precision, recall, f1 scores in Pytorch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, average: str = 'macro', pos_label: int = 1):\n",
    "        \"\"\"\n",
    "        Init.\n",
    "\n",
    "        Args:\n",
    "            average: averaging method\n",
    "        \"\"\"\n",
    "        self.average = average\n",
    "        self.pos_label = pos_label\n",
    "        if average not in [None, 'micro', 'macro', 'weighted', 'binary']:\n",
    "            raise ValueError('Wrong value of average parameter')\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_f1_micro(predictions: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calculate f1 micro.\n",
    "\n",
    "        Args:\n",
    "            predictions: tensor with predictions\n",
    "            labels: tensor with original labels\n",
    "\n",
    "        Returns:\n",
    "            f1 score\n",
    "        \"\"\"\n",
    "        true_positive = torch.eq(labels, predictions).sum().float()\n",
    "        f1_score = torch.div(true_positive, len(labels))  # micro f1 = micro precision = micro recall = avg\n",
    "        return f1_score, f1_score, f1_score\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_prf_count_for_label(labels: torch.Tensor, predictions: torch.Tensor, label_id: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Calculate precision, recall, f1 and true count for the label\n",
    "\n",
    "        Args:\n",
    "            labels: tensor with original labels\n",
    "            predictions: tensor with predictions\n",
    "            label_id: id of current label\n",
    "\n",
    "        Returns:\n",
    "            f1 score and true count for label\n",
    "        \"\"\"\n",
    "        # label count\n",
    "        true_count = torch.eq(labels, label_id).sum()\n",
    "\n",
    "        # true positives: labels equal to prediction and to label_id\n",
    "        true_positive = torch.logical_and(torch.eq(labels, predictions),\n",
    "                                          torch.eq(labels, label_id)).sum().float()\n",
    "        # precision for label\n",
    "        precision = torch.div(true_positive, torch.eq(predictions, label_id).sum().float())\n",
    "        # replace nan values with 0\n",
    "        precision = torch.where(torch.isnan(precision),\n",
    "                                torch.zeros_like(precision).type_as(true_positive),\n",
    "                                precision)\n",
    "\n",
    "        # recall for label\n",
    "        recall = torch.div(true_positive, true_count)\n",
    "        # f1\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "        # replace nan values with 0\n",
    "        f1 = torch.where(torch.isnan(f1), torch.zeros_like(f1).type_as(true_positive), f1)\n",
    "        return precision, recall, f1, true_count\n",
    "\n",
    "    def __call__(self, labels: torch.Tensor, predictions: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Calculate f1 score based on averaging method defined in init.\n",
    "\n",
    "        Args:\n",
    "            predictions: tensor with predictions\n",
    "            labels: tensor with original labels\n",
    "\n",
    "        Returns:\n",
    "            f1 score\n",
    "        \"\"\"\n",
    "        assert labels.dim() == 1, \"Flatten labels first!\"\n",
    "        assert predictions.dim() == 1, \"Flatten predictions first!\"\n",
    "\n",
    "        # simpler calculation for micro\n",
    "        if self.average == 'micro':\n",
    "            return self.calc_f1_micro(labels, predictions)\n",
    "        if self.average == 'binary':\n",
    "            p, r, f1, _ = self.calc_prf_count_for_label(labels, predictions, self.pos_label)\n",
    "            return p, r, f1\n",
    "\n",
    "        scores = torch.zeros(3)\n",
    "        for label_id in range(0, len(labels.unique())):\n",
    "            p, r, f1, true_count = self.calc_prf_count_for_label(labels, predictions, label_id)\n",
    "\n",
    "            if self.average == 'weighted':\n",
    "                scores += torch.tensor([p, r, f1]) * true_count\n",
    "            elif self.average == 'macro':\n",
    "                scores += torch.tensor([p, r, f1])\n",
    "\n",
    "        if self.average == 'weighted':\n",
    "            scores = scores / len(labels)\n",
    "        elif self.average == 'macro':\n",
    "            scores = scores / len(labels.unique())\n",
    "\n",
    "        return scores[0], scores[1], scores[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "560f80f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import numpy as np\n",
    "for _ in range(10):\n",
    "    labels = torch.randint(0, 10, (4096, 100)).flatten()\n",
    "    predictions = torch.randint(0, 10, (4096, 100)).flatten()\n",
    "\n",
    "    for av in ['macro', 'weighted', 'micro']:\n",
    "        my_p, my_r, my_f1 = PRFScore(av)(labels, predictions)\n",
    "\n",
    "        p, r, f1, _ = precision_recall_fscore_support(labels, predictions, average=av)\n",
    "        e_f1 = f1_score(labels, predictions, average=av)\n",
    "        assert np.isclose(my_p.item(), p)\n",
    "        assert np.isclose(my_r.item(), r)\n",
    "        assert np.isclose(my_f1.item(), f1)\n",
    "        assert np.isclose(my_f1.item(), e_f1)\n",
    "\n",
    "    labels = torch.randint(0, 2, (4096, 100)).flatten()\n",
    "    predictions = torch.randint(0, 2, (4096, 100)).flatten()\n",
    "    prf_metric = PRFScore(\"binary\")\n",
    "    my_p, my_r, my_f1 = prf_metric(labels, predictions)\n",
    "\n",
    "    p, r, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"binary\")\n",
    "    assert np.isclose(my_p.item(), p)\n",
    "    assert np.isclose(my_r.item(), r)\n",
    "    assert np.isclose(my_f1.item(), f1)\n",
    "print(\"tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457c2c03",
   "metadata": {},
   "source": [
    "### Setting Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bd2569f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56857c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd339228",
   "metadata": {},
   "source": [
    "### Reduce tensorboard training repeats with [`tb-reducer`](https://github.com/janosh/tensorboard-reducer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c63f14",
   "metadata": {},
   "source": [
    "```\n",
    "#!/usr/bin/env bash\n",
    "set -e\n",
    "ipd=out/movies_320/fr/supervised_cat\n",
    "opd=$ipd\n",
    "\n",
    "# rm -rf $opd\n",
    "tb-reducer -i \"$ipd/*\" -o $opd/ -r mean --lax-steps -f --lax-tags\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7621b455",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e39a79",
   "metadata": {},
   "source": [
    "### Gradient Clipping\n",
    "Useful for RNNs which are known to suffer from exploding gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4be2e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.utils.clip_grad_norm_(model.parameters(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7ef331",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "normalize_gradient(model)\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000fc84d",
   "metadata": {},
   "source": [
    "### BERT Word Embeddings\n",
    "Extended [this tutorial](https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/) that extracts wordpiece embeddings to extract word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36d97f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "from torch import Tensor\n",
    "import nltk\n",
    "\n",
    "def get_token_embeddings(string, tokenizer, embedding_model, merge_strategy = \"average\"):\n",
    "    \"\"\" Retrieves token embeddings by accumulating wordpiece embeddings based on merge strategy.\n",
    "    Identify wordpiece to tokens by checking if their character span is in subset of the original token char span. \"\"\"\n",
    "    def _merge_embeddings(wp_e, stack):\n",
    "        if merge_strategy == \"average\": \n",
    "            t_e = torch.mean(wp_e[stack], 0)\n",
    "        elif merge_strategy == \"first\":\n",
    "            t_e = wp_e[stack[0]]\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return t_e\n",
    "\n",
    "\n",
    "    inputs = tokenizer(string, truncation=True, return_tensors=\"pt\", add_special_tokens = False)\n",
    "    wp_e = get_wordpiece_embeddings(inputs, embedding_model)\n",
    "\n",
    "    ws_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "    token_spans = ws_tokenizer.span_tokenize(string)\n",
    "\n",
    "    # merging wordpiece embeddings\n",
    "    result = []\n",
    "    stack = []  # initialise stack with idx\n",
    "\n",
    "    t_span = next(token_spans)\n",
    "    for i in range(len(wp_e)):\n",
    "        wp_span = inputs.token_to_chars(i)\n",
    "        if is_subspan(wp_span, t_span): stack.append(i)\n",
    "        else: \n",
    "            t_e = _merge_embeddings(wp_e, stack)\n",
    "            result.append(t_e)\n",
    "            t_span = next(token_spans) # if error is thrown, sth is wrong as every wp should be a subspan of some token\n",
    "\n",
    "            assert is_subspan(wp_span, t_span)\n",
    "            stack = [i]  # initialise stack with current idx\n",
    "    \n",
    "    # clear remaining accumulated tensors\n",
    "    t_e = _merge_embeddings(wp_e, stack)\n",
    "    result.append(t_e)\n",
    "\n",
    "    result = torch.stack(result, dim = 0)\n",
    "    assert len(result) == len(string.split()), f\"{len(result)} != {len(string.split())}\"\n",
    "    return result\n",
    "\n",
    "def is_subspan(subspan: Tuple[int], span: Tuple[int]) -> bool:\n",
    "    assert len(subspan) == 2\n",
    "    assert len(span) == 2\n",
    "    return subspan[0] >= span[0] and subspan[1] <= span[1]\n",
    "\n",
    "def get_model_device(model):\n",
    "    return next(model.parameters()).device\n",
    "\n",
    "def get_wordpiece_embeddings(inputs: Dict, model, layer_merge_strategy==\"cat\") -> Tensor:\n",
    "    \"\"\" Based on https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/#2-input-formatting \"\"\"\n",
    "    with torch.no_grad():\n",
    "        inputs = {k: v.to(get_model_device(model)) for k, v in inputs.items()}\n",
    "        outputs = model(**inputs)\n",
    "        hidden_states = outputs[2]  # 13x1xtx768\n",
    "\n",
    "    # stack list of tensors \n",
    "    wordpiece_embeddings = torch.stack(hidden_states[-4:], dim = 0)  # 4x1xtx768\n",
    "\n",
    "    # remove batch dimension\n",
    "    wordpiece_embeddings = torch.squeeze(wordpiece_embeddings, dim = 1)  # 4xtx768\n",
    "\n",
    "    # order by wordpiece tokens\n",
    "    wordpiece_embeddings = wordpiece_embeddings.permute(1, 0, 2)  # tx4x768\n",
    "\n",
    "    if layer_merge_strategy == \"second-to-last\":\n",
    "        wordpiece_embeddings = wordpiece_embeddings[:, -2]\n",
    "    elif layer_merge_strategy == \"weighted_sum\":\n",
    "        wordpiece_embeddings = wordpiece_embeddings.sum(dim=1)  # tx768\n",
    "    elif layer_merge_strategy == \"cat\":  # best results according to BERT's paper\n",
    "        s = wordpiece_embeddings.size()\n",
    "        wordpiece_embeddings = wordpiece_embeddings.reshape(s[0], s[1] * s[2])  # tx3072\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    return wordpiece_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64a94a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_get_wordpiece_embeddings():\n",
    "    from transformers import AutoModel\n",
    "    from transformers import AutoTokenizer\n",
    "    \"\"\" Tests for contextual embeddings. Follows https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/#2-input-formatting. \"\"\"\n",
    "\n",
    "    from scipy.spatial.distance import cosine\n",
    "\n",
    "    # model_name = 'bert-base-uncased'\n",
    "    model_name = 'bert-base-multilingual-cased'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name, output_hidden_states = True)\n",
    "    model.eval()\n",
    "    \n",
    "    # english\n",
    "    en = \"After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.\"\n",
    "    inputs = tokenizer(en, return_tensors=\"pt\", truncation=True, add_special_tokens=False)\n",
    "    we_en = get_wordpiece_embeddings(inputs, model)\n",
    "\n",
    "    tokens = tokenizer.tokenize(en, truncation=True, add_special_tokens=False)\n",
    "    assert len(we_en) == len(tokens)\n",
    "    ids = [i for i, x in enumerate(tokens) if x == \"bank\"]\n",
    "\n",
    "    same_bank = 1 - cosine(we_en[ids[0]], we_en[ids[1]])\n",
    "    diff_bank = 1 - cosine(we_en[ids[0]], we_en[ids[2]])\n",
    "    print(f\"diff_bank vs same_bank for en: {diff_bank} vs {same_bank}\")\n",
    "    assert same_bank > diff_bank\n",
    "\n",
    "def test_get_token_embeddings():\n",
    "    from transformers import AutoModel\n",
    "    from transformers import AutoTokenizer\n",
    "    \"\"\" Tests for contextual embeddings. Follows https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/#2-input-formatting. \"\"\"\n",
    "\n",
    "    from scipy.spatial.distance import cosine\n",
    "\n",
    "    # model_name = 'bert-base-uncased'\n",
    "    model_name = 'bert-base-multilingual-cased'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name, output_hidden_states = True)\n",
    "    model.eval()\n",
    "    \n",
    "    # english\n",
    "    en = \"After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.\"\n",
    "    t_en = get_token_embeddings(en, tokenizer, model, merge_strategy=\"first\")\n",
    "\n",
    "    ids = [i for i, x in enumerate(en.split()) if \"bank\" in x]\n",
    "\n",
    "    same_bank = 1 - cosine(t_en[ids[0]], t_en[ids[1]])  # cosine similarity\n",
    "    diff_bank = 1 - cosine(t_en[ids[0]], t_en[ids[2]])\n",
    "    print(f\"diff_bank vs same_bank for en: {diff_bank} vs {same_bank}\")\n",
    "    assert same_bank > diff_bank\n",
    "\n",
    "    inputs = tokenizer(en, return_tensors=\"pt\", truncation=True, add_special_tokens=False)\n",
    "    we_en = get_wordpiece_embeddings(inputs, model)\n",
    "\n",
    "    tokens = tokenizer.tokenize(en, truncation=True, add_special_tokens=False)\n",
    "    assert len(we_en) == len(tokens)\n",
    "    we_ids = [i for i, x in enumerate(tokens) if x == \"bank\"]\n",
    "    # token and wordpiece embeddings should be same since \"bank\" is not split further.\n",
    "    assert torch.equal(t_en[ids[0]], we_en[we_ids[0]])\n",
    "    assert torch.equal(t_en[ids[1]], we_en[we_ids[1]])\n",
    "    assert torch.equal(t_en[ids[2]], we_en[we_ids[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e09c8a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff_bank vs same_bank for en: 0.8148452043533325 vs 0.9431322813034058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff_bank vs same_bank for en: 0.8148452043533325 vs 0.9431322813034058\n",
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "test_get_wordpiece_embeddings()\n",
    "test_get_token_embeddings()\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3c942c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
